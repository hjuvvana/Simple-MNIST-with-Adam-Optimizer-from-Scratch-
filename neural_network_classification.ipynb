{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caba3b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c1608ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juvva\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(name='mnist_784', version=1)\n",
    "X, Y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X / 255.\n",
    "\n",
    "# Shuffle and split the dataset into training and development sets\n",
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "m_train, n = X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml(name='mnist_784', version=1)\n",
    "X, Y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize pixel values\n",
    "X = X / 255.\n",
    "\n",
    "# Shuffle and split the dataset into training and development sets\n",
    "X, Y = shuffle(X, Y, random_state=42)\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "m_train, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a075e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Development Set: 0.8867142857142857\n"
     ]
    }
   ],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def adam_optimizer(W1, b1, W2, b2, dW1, db1, dW2, db2, m_W1, v_W1, m_b1, v_b1, m_W2, v_W2, m_b2, v_b2, t, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    m_W1 = beta1 * m_W1 + (1 - beta1) * dW1\n",
    "    v_W1 = beta2 * v_W1 + (1 - beta2) * (dW1 ** 2)\n",
    "    m_b1 = beta1 * m_b1 + (1 - beta1) * db1\n",
    "    v_b1 = beta2 * v_b1 + (1 - beta2) * (db1 ** 2)\n",
    "    m_W2 = beta1 * m_W2 + (1 - beta1) * dW2\n",
    "    v_W2 = beta2 * v_W2 + (1 - beta2) * (dW2 ** 2)\n",
    "    m_b2 = beta1 * m_b2 + (1 - beta1) * db2\n",
    "    v_b2 = beta2 * v_b2 + (1 - beta2) * (db2 ** 2)\n",
    "\n",
    "    m_W1_hat = m_W1 / (1 - beta1 ** t)\n",
    "    v_W1_hat = v_W1 / (1 - beta2 ** t)\n",
    "    m_b1_hat = m_b1 / (1 - beta1 ** t)\n",
    "    v_b1_hat = v_b1 / (1 - beta2 ** t)\n",
    "    m_W2_hat = m_W2 / (1 - beta1 ** t)\n",
    "    v_W2_hat = v_W2 / (1 - beta2 ** t)\n",
    "    m_b2_hat = m_b2 / (1 - beta1 ** t)\n",
    "    v_b2_hat = v_b2 / (1 - beta2 ** t)\n",
    "\n",
    "    W1 = W1 - alpha * m_W1_hat / (np.sqrt(v_W1_hat) + epsilon)\n",
    "    b1 = b1 - alpha * m_b1_hat / (np.sqrt(v_b1_hat) + epsilon)\n",
    "    W2 = W2 - alpha * m_W2_hat / (np.sqrt(v_W2_hat) + epsilon)\n",
    "    b2 = b2 - alpha * m_b2_hat / (np.sqrt(v_b2_hat) + epsilon)\n",
    "\n",
    "    return W1, b1, W2, b2, m_W1, v_W1, m_b1, v_b1, m_W2, v_W2, m_b2, v_b2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "\n",
    "\n",
    "# Gradient descent optimization\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    m_W1, v_W1, m_b1, v_b1 = np.zeros_like(W1), np.zeros_like(W1), np.zeros_like(b1), np.zeros_like(b1)\n",
    "    m_W2, v_W2, m_b2, v_b2 = np.zeros_like(W2), np.zeros_like(W2), np.zeros_like(b2), np.zeros_like(b2)\n",
    "\n",
    "    for i in range(1, iterations + 1):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2, m_W1, v_W1, m_b1, v_b1, m_W2, v_W2, m_b2, v_b2 = adam_optimizer(\n",
    "            W1, b1, W2, b2, dW1, db1, dW2, db2,\n",
    "            m_W1, v_W1, m_b1, v_b1, m_W2, v_W2, m_b2, v_b2,\n",
    "            i, alpha=alpha\n",
    "        )\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Run the gradient descent optimization\n",
    "W1, b1, W2, b2 = gradient_descent(X_train.T, Y_train, alpha=0.001, iterations=500)\n",
    "\n",
    "# Evaluate the accuracy on the development set\n",
    "dev_predictions = get_predictions(forward_prop(W1, b1, W2, b2, X_dev.T)[-1])\n",
    "dev_accuracy = get_accuracy(dev_predictions, Y_dev)\n",
    "print(f\"Accuracy on Development Set: {dev_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
